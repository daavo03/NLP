{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1Rh_c1Z888nf0UkzKJy_5gvKDDPbHPxAt",
      "authorship_tag": "ABX9TyPKGrIzhpiio9PBi2RSM1nN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daavo03/NLP/blob/main/Rule_based_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp7ixC_wWTrY"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "tZHaydHeYhmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import random\n",
        "import re, unicodedata\n",
        "import warnings\n",
        "import wikipedia as wk\n",
        "\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "OsEVVluMWqnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download datasets"
      ],
      "metadata": {
        "id": "o8zIN2XjZoB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-tZkEK2ZnMU",
        "outputId": "3de01e85-d0e1-4e4d-eed6-02b46d2f0488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input data to use"
      ],
      "metadata": {
        "id": "0RTixYh0aY_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the data and makesure there's no capital letters in the data\n",
        "data = open('./drive/MyDrive/colab_files/HR.txt', 'r', errors='ignore')\n",
        "raw = data.read()\n",
        "raw = raw.lower()"
      ],
      "metadata": {
        "id": "ZuuYioMgaVWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eJAjPAbubD4i",
        "outputId": "914ea295-4b29-45a7-bb94-9f958921521c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'human resource analytics is at the intersection of three bodies of knowledge:\\n\\nhuman resource manage'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize the data"
      ],
      "metadata": {
        "id": "DqtB7wF5byYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens = nltk.sent_tokenize(raw)"
      ],
      "metadata": {
        "id": "1fCUZ8uCgy8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "    # Remove the puntuaction dictionary\n",
        "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "    # Develop the word tokenization\n",
        "    word_token = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
        "\n",
        "    # Remove ascii\n",
        "    new_words = []\n",
        "    for word in word_token:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "\n",
        "    # Remove tags\n",
        "    rmv = []\n",
        "    for w in new_words:\n",
        "        text = re.sub(\"&lt;/?.*?&gt;\", \"&lt;&gt;\", w)\n",
        "        rmv.append(text)\n",
        "\n",
        "    # Lemmatization\n",
        "    tag_map = defaultdict(lambda:wn.NOUN)\n",
        "    tag_map['J'] = wn.ADJ\n",
        "    tag_map['V'] = wn.VERB\n",
        "    tag_map['R'] = wn.ADV\n",
        "\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    lemma_list = []\n",
        "    rmv = [i for i in rmv if i]\n",
        "    for token, tag in nltk.pos_tag(rmv):\n",
        "        lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
        "        lemma_list.append(lemma)\n",
        "\n",
        "    return lemma_list\n",
        "\n"
      ],
      "metadata": {
        "id": "l9QNkZMZhK0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "welcome_input = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
        "welcome_response = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "\n",
        "def welcome(user_response):\n",
        "    for word in user_response.split():\n",
        "        if word.lower() in welcome_input:\n",
        "            return random.choice(welcome_response)"
      ],
      "metadata": {
        "id": "gH333NxxoVOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateResponse(user_response):\n",
        "    robo_response = ''\n",
        "    sent_tokens.append(user_response)\n",
        "\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "\n",
        "    vals = linear_kernel(tfidf[-1], tfidf)\n",
        "    idx = vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "\n",
        "    if (req_tfidf == 0) or \"tell me about\" in user_response:\n",
        "        print(\"Checking Wikipedia\")\n",
        "        if user_response:\n",
        "            robo_response = wikipedia_data(user_response)\n",
        "            return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "\n",
        "def wikipedia_data(input):\n",
        "    reg_ex = re.search('tell me about(.*)', input)\n",
        "\n",
        "    try:\n",
        "        if reg_ex:\n",
        "            topic = reg_ex.group(1)\n",
        "            wiki = wk.summary(topic, sentences=3)\n",
        "            return wiki\n",
        "    except Exception as e:\n",
        "        print(\"No content available\")"
      ],
      "metadata": {
        "id": "2okVC3scpQFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print(\"My name us chatty and I'm a chatbot\")\n",
        "\n",
        "while flag:\n",
        "    user_response = input().lower()\n",
        "\n",
        "    if user_response not in ['bye', 'shutdown', 'exit', 'quit']:\n",
        "        if (user_response == 'thanks' or user_response=='thank you'):\n",
        "            flag = False\n",
        "            print(\"Chattybot : You'r welcome\")\n",
        "        else:\n",
        "            if(welcome(user_response) != None):\n",
        "                print(\"Chattybot : \"+welcome(user_response))\n",
        "            else:\n",
        "                print(\"Chattybot : \", end=\"\")\n",
        "                print(generateResponse(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag = False\n",
        "        print(\"Chattybot : Bye!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w74WNVjrWhb",
        "outputId": "443dbdf8-ee66-4a64-c0ad-3e874f00a800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name us chatty and I'm a chatbot\n",
            "daavo\n",
            "Chattybot : Checking Wikipedia\n",
            "None\n",
            "hola\n",
            "Chattybot : Checking Wikipedia\n",
            "None\n",
            "What is HR\n",
            "Chattybot : hr metrics: application of formulas for measuring and calculating core hr issues so as to draw exact hr results and current scenario of organisation.\n",
            "how do they work?\n",
            "Chattybot : creating a system to coordinate the works of the members so as to make the employees to work properly and not to cause any conflict in the allocation of the work to the employees.\n",
            "tell me about\n",
            "Chattybot : Checking Wikipedia\n",
            "No content available\n",
            "None\n",
            "Maths\n",
            "Chattybot : Checking Wikipedia\n",
            "None\n",
            "tell me about Friends\n",
            "Chattybot : Checking Wikipedia\n",
            "Friends is an American television sitcom created by David Crane and Marta Kauffman, which aired on NBC from September 22, 1994, to May 6, 2004, lasting ten seasons. With an ensemble cast starring Jennifer Aniston, Courteney Cox, Lisa Kudrow, Matt LeBlanc, Matthew Perry and David Schwimmer, the show revolves around six friends in their 20s and early 30s who live in Manhattan, New York City. The original executive producers were Kevin S. Bright, Kauffman, and Crane.\n",
            "quit\n",
            "Chattybot : Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PCZwQAQnsexz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}